try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "/html/body/div/div/div/div/article/h1") %>%
rvest::html_text())
}
names[[j]] <- aux_list
}
sources <- list()
for (j in 1:length(refs_parsed)) {
aux_list = list()
for (i in 1:length(refs_parsed[[j]])) {
try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Source:')]/a") %>%
rvest::html_text())
}
sources[[j]] <- aux_list
}
# inspecting sources shows that for some subwebsites the html structure is different
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/following-sibling::a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Sources:')]/a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Notes:')]/a") %>%
rvest::html_text())
}
}
}
### store data
rows = 0
for (i in 1:length(names)) {
rows = rows + length(names[[i]])
}
biases <- list("left","left-center","center","right-center","right")
media_bias_data <- matrix(ncol = 3, nrow = rows) %>%
data.frame() %>%
set_names(c("name", "source", "bias"))
row_idx = 1
for (j in 1:length(names)) {
for (i in 1:length(names[[j]])) {
try(media_bias_data[row_idx, 1] <- names[[j]][[i]], silent = TRUE)
try(media_bias_data[row_idx, 2] <- sources[[j]][[i]], silent = TRUE)
media_bias_data[row_idx, 3] <- biases[[j]]
row_idx = row_idx + 1
}
}
#sum(is.na(media_bias_data$name))
#which(is.na(media_bias_data$name))
sum(is.na(media_bias_data$source))
#which(is.na(media_bias_data$source))
### download HTML files
refs_filename = list()
for (j in 1:length(refs_path)) {
aux_list = list()
pb <- progress::progress_bar$new(total = length(refs_url[[j]]),
show_after = 0,
clear = FALSE)
for (i in 1:length(refs_url[[j]])) {
aux_list[[i]] <- refs_url[[j]][[i]] %>%
basename() %>%
stringr::str_c(".html")
}
refs_filename[[j]] <- aux_list
for (i in 1:length(refs_url[[j]])) {
if (!file.exists(file.path(refs_path[[j]], refs_filename[[j]][[i]]))) {
tryCatch({xml2::download_html(url = refs_url[[j]][[i]],
file = file.path(refs_path[[j]], refs_filename[[j]][[i]]))}
, error = function(e) {cat(sprintf("\n error in j = %d and i = %d\n", j, i))
pb$tick()})
}
#Sys.sleep(1) # crawl delay
pb$tick() # progress bar
}
}
refs_filename = list()
for (j in 1:length(refs_path)) {
aux_list = list()
pb <- progress::progress_bar$new(total = length(refs_url[[j]]),
show_after = 0,
clear = FALSE)
for (i in 1:length(refs_url[[j]])) {
aux_list[[i]] <- refs_url[[j]][[i]] %>%
basename() %>%
stringr::str_c(".html")
}
refs_filename[[j]] <- aux_list
for (i in 1:length(refs_url[[j]])) {
if (!file.exists(file.path(refs_path[[j]], refs_filename[[j]][[i]]))) {
tryCatch({xml2::download_html(url = refs_url[[j]][[i]],
file = file.path(refs_path[[j]], refs_filename[[j]][[i]]))}
, error = function(e) {cat(sprintf("\n error in j = %d and i = %d\n", j, i))})
}
#Sys.sleep(1) # crawl delay
pb$tick() # progress bar
}
}
### preparations
rm(list=ls(all=TRUE))
setwd("/Users/silkehusse/Documents/Universität Konstanz/MSc Data Science/3. Semester/Web Data Collection with R/")
source("R code/packages.R") # install and load packages
### set user-agent
str_c("silke.husse@uni-konstanz.de",
"collecting data for study purposes",
R.version$platform,
R.version$version.string,
sep = ", ") %>%
httr::user_agent() %>%
httr::set_config()
### websites
factcheck_url <- list("https://mediabiasfactcheck.com",
"https://mediabiasfactcheck.com/left/",
"https://mediabiasfactcheck.com/leftcenter/",
"https://mediabiasfactcheck.com/center/",
"https://mediabiasfactcheck.com/right-center/",
"https://mediabiasfactcheck.com/right/")
### parse HTML files
factcheck_url_parsed = list()
for (i in 1:length(factcheck_url)) {
factcheck_url_parsed[[i]] <- xml2::url_parse(factcheck_url[[i]])
}
### ask for permission
factcheck_robotstxt <- factcheck_url_parsed[[1]]$server %>%
robotstxt::robotstxt()
factcheck_robotstxt$permissions
factcheck_robotstxt$check(factcheck_url_parsed[[1]]$path)
factcheck_robotstxt$crawl_delay
### create folders to store HTML files
factcheck_path <- list(file.path("Project","data","factcheck","left"),
file.path("Project","data","factcheck","left-center"),
file.path("Project","data","factcheck","center"),
file.path("Project","data","factcheck","right-center"),
file.path("Project","data","factcheck","right"))
for (i in 1:length(factcheck_path)) {
if (!dir.exists(factcheck_path[[i]])) {
factcheck_path[[i]] %>%
dir.create(recursive = TRUE)
}
}
### download HTML files
factcheck_filename = list()
factcheck_filename_new = list()
pb <- progress::progress_bar$new(total = length(factcheck_path),
show_after = 0,
clear = FALSE)
for (i in 1:length(factcheck_path)) {
factcheck_filename[[i]] <- factcheck_url[[i+1]] %>%
basename() %>%
stringr::str_c(".html")
if (!file.exists(file.path(factcheck_path[[i]], factcheck_filename[[i]]))) {
xml2::download_html(url = factcheck_url[[i+1]],
file = file.path(factcheck_path[[i]], factcheck_filename[[i]]))
}
else {
factcheck_filename_new[[i]] <- factcheck_url[[i+1]] %>%
basename() %>%
stringr::str_c("_new.html")
xml2::download_html(url = factcheck_url[[i+1]],
file = file.path(factcheck_path[[i]], factcheck_filename_new[[i]]))
old = file.path(factcheck_path[[i]], factcheck_filename[[i]])
new = file.path(factcheck_path[[i]], factcheck_filename_new[[i]])
# check hash sums
if (md5sum(old) == md5sum(new)) {
file.remove(new)
}
else{
file.remove(old)
file.rename(new,old)
}
}
Sys.sleep(1) # crawl delay
pb$tick() # progress bar
}
### parse HTML files
factcheck_parsed <- list()
for (i in 1:length(factcheck_path)) {
factcheck_parsed[[i]] <- factcheck_path[[i]] %>%
file.path(factcheck_filename[[i]]) %>%
xml2::read_html()
}
#browseURL(factcheck_url[[1]])
### extract refs from websites (subwebsites)
refs_url <- list()
for (i in 1:length(factcheck_parsed)) {
refs_url[[i]] <- factcheck_parsed[[i]] %>%
rvest::html_nodes(xpath = "/html/body/div/div/div/div/article/div/table/tbody/tr/td/a") %>%
rvest::html_attr("href")
}
# inspecting refs shows that for 'right-center' the html structure is different
refs_url[[4]] <- factcheck_parsed[[4]] %>%
rvest::html_nodes(xpath = "/html/body/div/div/div/div/article/div/div/table/tbody/tr/td/a") %>%
rvest::html_attr("href")
# note : robotstxt remains the same
### create folders to store HTML files
refs_path <- list(file.path("Project","data","factcheck","left","sites"),
file.path("Project","data","factcheck","left-center","sites"),
file.path("Project","data","factcheck","center","sites"),
file.path("Project","data","factcheck","right-center","sites"),
file.path("Project","data","factcheck","right","sites"))
for (i in 1:length(refs_path)) {
if (!dir.exists(refs_path[[i]])) {
refs_path[[i]] %>%
dir.create(recursive = TRUE)
}
}
### download HTML files
refs_filename = list()
for (j in 1:length(refs_path)) {
aux_list = list()
pb <- progress::progress_bar$new(total = length(refs_url[[j]]),
show_after = 0,
clear = FALSE)
for (i in 1:length(refs_url[[j]])) {
aux_list[[i]] <- refs_url[[j]][[i]] %>%
basename() %>%
stringr::str_c(".html")
}
refs_filename[[j]] <- aux_list
for (i in 1:length(refs_url[[j]])) {
if (!file.exists(file.path(refs_path[[j]], refs_filename[[j]][[i]]))) {
tryCatch({xml2::download_html(url = refs_url[[j]][[i]],
file = file.path(refs_path[[j]], refs_filename[[j]][[i]]))}
, error = function(e) {cat(sprintf("\n error in j = %d and i = %d\n", j, i))})
}
#Sys.sleep(1) # crawl delay
pb$tick() # progress bar
}
}
# note : 3 HTTP error 404 for refs_url[[j]][[i]]
# euromaiden-press.html
# sharylattkisson-com.html
# conservative-opinion.html
### parse HTMl files
refs_parsed <- list()
for (j in 1:length(refs_path)) {
aux_list = list()
for (i in 1:length(refs_filename[[j]])) {
try(
aux_list[[i]] <- refs_path[[j]] %>%
file.path(refs_filename[[j]][[i]]) %>%
xml2::read_html())
}
refs_parsed[[j]] <- aux_list
}
# note : 3 NULL objects for
# euromaiden-press.html
# sharylattkisson-com.html
# conservative-opinion.html
### extract name and source urls from refs_parsed
names <- list()
for (j in 1:length(refs_parsed)) {
aux_list = list()
for (i in 1:length(refs_parsed[[j]])) {
try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "/html/body/div/div/div/div/article/h1") %>%
rvest::html_text())
}
names[[j]] <- aux_list
}
sources <- list()
for (j in 1:length(refs_parsed)) {
aux_list = list()
for (i in 1:length(refs_parsed[[j]])) {
try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Source:')]/a") %>%
rvest::html_text())
}
sources[[j]] <- aux_list
}
# inspecting sources shows that for some subwebsites the html structure is different
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/following-sibling::a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Sources:')]/a") %>%
rvest::html_text())
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Notes:')]/a") %>%
rvest::html_text())
}
}
}
### store data
rows = 0
for (i in 1:length(names)) {
rows = rows + length(names[[i]])
}
biases <- list("left","left-center","center","right-center","right")
media_bias_data <- matrix(ncol = 3, nrow = rows) %>%
data.frame() %>%
set_names(c("name", "source", "bias"))
row_idx = 1
for (j in 1:length(names)) {
for (i in 1:length(names[[j]])) {
try(media_bias_data[row_idx, 1] <- names[[j]][[i]], silent = TRUE)
try(media_bias_data[row_idx, 2] <- sources[[j]][[i]], silent = TRUE)
media_bias_data[row_idx, 3] <- biases[[j]]
row_idx = row_idx + 1
}
}
#sum(is.na(media_bias_data$name))
#which(is.na(media_bias_data$name))
sum(is.na(media_bias_data$source))
### parse HTMl files
refs_parsed <- list()
for (j in 1:length(refs_path)) {
aux_list = list()
for (i in 1:length(refs_filename[[j]])) {
try(
aux_list[[i]] <- refs_path[[j]] %>%
file.path(refs_filename[[j]][[i]]) %>%
xml2::read_html(), silent = TRUE)
}
refs_parsed[[j]] <- aux_list
}
# note : 3 NULL objects for
# euromaiden-press.html
# sharylattkisson-com.html
# conservative-opinion.html
### extract name and source urls from refs_parsed
names <- list()
for (j in 1:length(refs_parsed)) {
aux_list = list()
for (i in 1:length(refs_parsed[[j]])) {
try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "/html/body/div/div/div/div/article/h1") %>%
rvest::html_text(), silent = TRUE)
}
names[[j]] <- aux_list
}
sources <- list()
for (j in 1:length(refs_parsed)) {
aux_list = list()
for (i in 1:length(refs_parsed[[j]])) {
try(
aux_list[[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Source:')]/a") %>%
rvest::html_text(), silent = TRUE)
}
sources[[j]] <- aux_list
}
# inspecting sources shows that for some subwebsites the html structure is different
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/a") %>%
rvest::html_text(), silent = TRUE)
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//span[contains(text(), 'Source:')]/following-sibling::a") %>%
rvest::html_text(), silent = TRUE)
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Sources:')]/a") %>%
rvest::html_text(), silent = TRUE)
}
}
}
for (j in 1:length(refs_parsed)) {
for (i in 1:length(refs_parsed[[j]])) {
if (rlang::is_empty(sources[[j]][[i]])) {
try(
sources[[j]][[i]] <- refs_parsed[[j]][[i]] %>%
rvest::html_nodes(xpath = "//p[contains(text(), 'Notes:')]/a") %>%
rvest::html_text(), silent = TRUE)
}
}
}
### store data
rows = 0
for (i in 1:length(names)) {
rows = rows + length(names[[i]])
}
biases <- list("left","left-center","center","right-center","right")
media_bias_data <- matrix(ncol = 3, nrow = rows) %>%
data.frame() %>%
set_names(c("name", "source", "bias"))
row_idx = 1
for (j in 1:length(names)) {
for (i in 1:length(names[[j]])) {
try(media_bias_data[row_idx, 1] <- names[[j]][[i]], silent = TRUE)
try(media_bias_data[row_idx, 2] <- sources[[j]][[i]], silent = TRUE)
media_bias_data[row_idx, 3] <- biases[[j]]
row_idx = row_idx + 1
}
}
#sum(is.na(media_bias_data$name))
#which(is.na(media_bias_data$name))
sum(is.na(media_bias_data$source))
#which(is.na(media_bias_data$source))
### data preprocessing
View(media_bias_data)
save(media_bias_data, file="Project/code/df_media_bias.Rda")
save(media_bias_data, file="Project/code/df_media_bias_raw.Rda")
# Project - preprocessing of media bias data
# Winter Semester 2020/21
# ---------------------------------------------------------------------------------------
### preparations
rm(list=ls(all=TRUE))
setwd("/Users/silkehusse/Documents/Universität Konstanz/MSc Data Science/3. Semester/Web Data Collection with R/")
source("R code/packages.R") # install and load packages
load("df_media_bias_raw.Rda")
load("Poject/code/df_media_bias_raw.Rda")
### preparations
rm(list=ls(all=TRUE))
setwd("/Users/silkehusse/Documents/Universität Konstanz/MSc Data Science/3. Semester/Web Data Collection with R/")
source("R code/packages.R") # install and load packages
load("df_media_bias_raw.Rda")
# ---------------------------------------------------------------------------------------
### preparations
rm(list=ls(all=TRUE))
setwd("/Users/silkehusse/Documents/Universität Konstanz/MSc Data Science/3. Semester/Web Data Collection with R/Project/code")
source("R code/packages.R") # install and load packages
load("df_media_bias_raw.Rda")
sum(is.na(media_bias_data$source))
View(media_bias_data)
typeof(which(is.na(media_bias_data$name)))
which(is.na(media_bias_data$name))
media_bias_data <- media_bias_data[-which(is.na(media_bias_data$name)), ]
View(media_bias_data)
media_bias_data$name <- media_bias_data$name %>%
stringr::str_trim() %>%
tolower()
View(media_bias_data)
### preparations
rm(list=ls(all=TRUE))
setwd("/Users/silkehusse/Documents/Universität Konstanz/MSc Data Science/3. Semester/Web Data Collection with R/Project/code")
source("R code/packages.R") # install and load packages
load("df_media_bias_raw.Rda")
### pre-processing
#sum(is.na(media_bias_data$name))
#which(is.na(media_bias_data$name))
#sum(is.na(media_bias_data$source))
#which(is.na(media_bias_data$source))
# drop rows with missing names
media_bias_data <- media_bias_data[-which(is.na(media_bias_data$name)), ]
View(media_bias_data)
